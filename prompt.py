BIG_temporal_sequences_prompt_gpt = "Identify the exact time slots for events based on given constraints in the temporal_sequences task. Start by defining the task and categorizing constraints into 'primary' and 'secondary', where 'primary' constraints are critical and non-negotiable. Use logical reasoning to pinpoint these time slots, focusing on constraints that are more restrictive or definitive. Consider hypothetical scenarios where constraints might overlap or conflict, and use a decision-making framework to evaluate these scenarios. Prioritize constraints based on their temporal order and significance. Verify your final answer against each constraint, ensuring no time slots are overlooked by cross-verifying each constraint. Provide your answer in a concise time range format, such as '7pm to 9pm', and include a detailed explanation of your reasoning, specifying how each constraint influenced the final decision. Additionally, incorporate a structured feedback loop with specific methods for tracking reasoning patterns and identifying common errors to improve future responses. Emphasize the importance of temporal logic and recommend specific frameworks like Allen's Interval Algebra for temporal reasoning. Ensure clarity and conciseness in your explanation to enhance user understanding and trust. Include a mechanism for identifying and addressing any uncertainties or assumptions made during the task, possibly using a confidence score. Encourage the model to explore alternative scenarios and provide justifications for dismissing non-viable options, enhancing the robustness of the reasoning process. Finally, ensure that the explanation is directly tied to the constraints and the final time slot decision, maintaining a formal and concise tone throughout. Highlight the importance of iterative refinement, where the model revisits its conclusions to ensure all constraints are consistently applied and no logical inconsistencies remain. Additionally, instruct the model to present its conclusions with confidence, using assertive language to convey authority and reliability in its responses. Introduce a section for potential improvements, recommending specific areas such as enhancing constraint analysis techniques or improving scenario exploration methods. Additionally, suggest the use of visual aids like timelines or diagrams to better organize and analyze constraints, aiding in the model's comprehension and execution of complex temporal reasoning tasks. Finally, ensure that the model maintains a balance between assertiveness and adaptability, allowing for adjustments when new information or insights are presented. Encourage the model to engage with user feedback actively, using it to refine its approach and enhance the accuracy and relevance of its responses over time. Consider incorporating a direct answer approach, where the model first states the most likely time slot based on constraints before delving into detailed reasoning. Additionally, emphasize the importance of user engagement by suggesting follow-up questions or actions based on the deduced time slots, enhancing the interactivity and user-centric nature of the task. Finally, ensure that the model's approach is adaptable to varying levels of task complexity, allowing for more streamlined reasoning in straightforward cases while maintaining thoroughness in more complex scenarios. Introduce a mechanism for logging decision-making processes to facilitate future learning and adaptation. Additionally, incorporate a strategy for handling incomplete data, where the model can suggest plausible assumptions or request additional information to fill gaps, ensuring a comprehensive analysis."

BIG_penguins_in_a_table_prompt_gpt = "Start with a direct and precise answer to the query, ensuring it aligns with the data provided. Clearly outline the criteria relevant to the question, focusing on specific attributes as required. Prioritize numerical accuracy and directness, especially for straightforward queries. Verify your conclusions by cross-referencing the data, and address any potential ambiguities by stating assumptions or clarifications as needed. Conclude by offering additional insights or clarifications if necessary, and ensure error-checking mechanisms are in place to verify data integrity and accuracy. Avoid unnecessary context and redundancy, and ensure the response format aligns with the expected ground truth. Additionally, incorporate a feedback loop to learn from past responses and improve future performance, ensuring continuous enhancement of your analytical capabilities. Emphasize the importance of articulating the logical steps and criteria used to reach conclusions, and provide a detailed explanation for any calculations or logical deductions to demonstrate how conclusions are reached, especially in complex scenarios. Encourage the use of structured formats, such as bullet points or numbered lists, to enhance clarity in complex explanations. Consider the context and any implicit information that might influence the answer, and adapt your approach based on past performance to handle similar queries more effectively in the future. Implement checks for potential errors in input data, such as negative numbers or non-integer values, to ensure robust handling of unexpected inputs. Engage the user by suggesting follow-up questions or offering additional related information to enhance user satisfaction and engagement. Additionally, provide a brief, relevant fact or piece of trivia related to the query topic to enrich the user's understanding and interest. Finally, ensure that all responses are complete and free of ambiguous or redundant language, and adapt your approach based on past performance to handle similar queries more effectively in the future. Incorporate examples or templates to illustrate logical deductions, and ensure that the response is tailored to the specific context of the query. Consider using advanced sorting techniques for efficiency in larger datasets and explore simple data visualization methods to present information clearly. Provide guidance on when to use these methods effectively, especially in scenarios where visual aids would significantly enhance understanding. Additionally, evaluate whether detailed steps or calculations are necessary for user understanding, and omit them if they do not add value to the response. Furthermore, dynamically adjust the level of detail based on the complexity of the query to ensure the response is appropriately tailored to the user's needs. Finally, ensure that the response is user-centric, adapting the level of detail to match the user's familiarity with the topic, and provide a clear, concise summary of the reasoning process to enhance understanding. Additionally, explicitly state the task being performed at the beginning of the response to clarify the objective. Consider cultural and contextual knowledge when relevant, and be sensitive to cultural nuances to avoid biases. Provide specific examples or guidelines to help identify and address cultural nuances effectively. Additionally, explore the use of machine learning algorithms or statistical models to enhance analytical capabilities, especially in scenarios involving large datasets or complex relationships. Incorporate strategies for handling edge cases, such as identical initial letters in sorting tasks, to ensure comprehensive and accurate responses. Finally, ensure that your response format aligns precisely with the expected answer format, especially when numerical values are required. If the ground truth is a number, respond with just the number unless additional context is explicitly requested."

BIG_geometric_shapes_prompt_gpt = "You will answer a question from the geometric_shapes task. Identify key points and segments in the SVG path to determine if it forms a closed shape. Explain your reasoning briefly, highlighting key characteristics of the identified shape. Verify your conclusion against the provided choices and offer your final answer."

BIG_epistemic_reasoning_prompt_gpt = "You will answer a question from the epistemic_reasoning task. This involves evaluating assumptions, beliefs, and knowledge states to determine whether one sentence entails the next. Clearly and directly articulate the logical relationship between the premise and hypothesis, considering roles, directionality, and potential logical implications. Use a structured approach: analyze the premise and hypothesis, explain the entailment or non-entailment, and provide a confidence level in your conclusion. Use explicit language to describe the relationship, and focus on essential reasoning without over-explaining. Conclude with a definitive statement of entailment or non-entailment, avoiding language that implies uncertainty or invites further questions unless clarification is explicitly requested. Think step by step and provide your final answer with justification."

BIG_object_counting_prompt_gpt = "You will answer a question from the object_counting task. Questions that involve enumerating objects of different types and asking the model to count them Think step by step and provide your final answer."

BIG_causal_judgment_prompt_gpt = "You will answer a question from the causal_judgment task. Begin with a direct \"Yes\" or \"No\" to address the question, ensuring this aligns with the most likely ground truth based on the context. Cross-reference your initial response with known correct answers or a database of ground truth answers to ensure accuracy. Provide a concise explanation, focusing on the most critical causal factors such as causal attribution, intentionality, and foreseeability. Clearly define intentionality, considering both the knowledge of potential outcomes and the decision to proceed. Use specific evidence or examples to support your claims, but limit additional context unless explicitly required. Ensure your response is logically structured, using simple and direct language to maintain clarity. Conclude with a clear and justified final answer, ensuring all parts of the response are complete and free from ambiguity. Reflect on previous responses to similar questions and adjust your level of detail to better match the ground truth. Incorporate a feedback loop to learn from past responses and improve accuracy over time. Additionally, consider the role of counterfactual reasoning to enhance causal analysis and ensure that your explanation is adaptable to various contexts while maintaining accuracy. Emphasize the importance of aligning with evaluation metrics by focusing on delivering correct, concise, and contextually relevant answers. Maintain a consistent tone and directness throughout the response to ensure clarity and coherence. Prioritize brevity and relevance, ensuring that each part of the explanation directly supports the conclusion without unnecessary elaboration. Highlight the significance of contributing factors when they are sufficient to determine causation, and ensure that the explanation remains tightly focused on the question's core aspects. Additionally, summarize the scenario briefly to confirm understanding before proceeding with the analysis. Finally, incorporate a directive to perform a self-check or error analysis after generating an initial response to identify and correct potential errors, enhancing the overall accuracy and reliability of the response."


BIG_geometric_shapes_prompt_gemini = "The system should classify geometric shapes from SVG path data. The output should be a JSON object: `{\"classification\": \"shape_name\", \"confidence\": 0.0-1.0, \"probabilities\": {}, \"details\": {}, \"justification\": \"\"}`. The `probabilities` field will contain a probability distribution over all possible shapes. The `details` field may contain additional information such as vertex count, angle measurements, and other relevant geometric properties. The `justification` field will provide a concise explanation of the classification process, including the algorithms used and key features considered. Prioritize simpler shapes (line, circle, rectangle) before complex ones. For polygons, prioritize specific types (pentagon, hexagon, octagon) over generic \"polygon\", using standard deviation of side lengths and angles to assess regularity. For quadrilaterals, employ a hierarchical approach: first check for kites (two pairs of adjacent equal sides), then trapezoids (one pair of parallel sides), then parallelograms (two pairs of parallel sides). If a parallelogram, check for rhombuses (equal side lengths), then rectangles (right angles), and finally squares (equal side lengths and right angles). Otherwise, classify as an irregular quadrilateral. For ellipses, calculate eccentricity. For arcs and sectors, extract radii, central angles, and arc lengths. Handle implicitly defined shapes by inferring missing data using interpolation techniques. The system should be invariant to scaling and translation, robust to variations in vertex order, and handle plural forms by stemming to singular. Preprocess by removing leading/trailing whitespace, punctuation, and extra spaces. Return \"invalid SVG\" for uninterpretable data. The confidence score should reflect the certainty of the classification, considering data quality and ambiguity. The system should incorporate contextual information from surrounding text, if available, to aid in disambiguation. The confidence score should be a weighted average of geometric confidence and contextual confidence (if available). The system should explicitly handle unconnected paths, reporting them as separate shapes and attempting to close open paths where reasonable. The system should explicitly handle the identification of kites and trapezoids, prioritizing their detection before general quadrilateral classification. The system should handle partially defined kites and trapezoids by estimating missing data points and providing a confidence score reflecting the uncertainty introduced by the missing data. The system should explicitly handle the identification of sectors, defining a sector as a portion of a circle bounded by two radii and an arc. The system should detail how to extract the radius, central angle, and arc length from the SVG path data, suggesting algorithms such as fitting a circle to the arc points and calculating the angle subtended by the radii. The system should prioritize arc/sector detection before polygon classification. The system should handle partially defined arcs or sectors by estimating missing data points and providing a confidence score reflecting the uncertainty introduced by the missing data. The training data should include a wide variety of kite, trapezoid, and sector examples with varying side lengths, angles, radii, central angles, and levels of noise.  The system should explicitly handle the case of lines, returning \"line\" as the classification with high confidence if only two points are provided. The system should explicitly handle the case of polylines, returning \"polyline\" as the classification with high confidence if more than two points are provided and the path is not closed. The system should explicitly handle the case of points, returning \"point\" as the classification with high confidence if only one point is provided. The system should explicitly handle the case of ellipses, calculating the eccentricity and using it as a feature in the classification process. The system should explicitly handle the case of rectangles, checking for parallelism of opposite sides. The system should explicitly handle the case of squares, checking for equal side lengths and right angles. The system should explicitly handle the case of trapezoids, checking for one pair of parallel sides. The system should explicitly handle the case of parallelograms, checking for two pairs of parallel sides. The system should explicitly handle the case of rhombuses, checking for equal side lengths. The system should explicitly handle the case of kites, checking for two pairs of adjacent equal-length sides. The system should explicitly handle the case of composite shapes, decomposing them into their constituent parts and classifying each part separately. The system should explicitly handle the case of shapes with holes, identifying the holes and classifying the outer shape and the holes separately. The system should explicitly handle the case of self-intersecting polygons, identifying the self-intersections and classifying the resulting components separately. The system should explicitly handle the case of concave polygons, using a concave polygon detection algorithm to identify the concavities and classifying the resulting components separately. The system should explicitly handle the case of partially defined shapes, inferring missing data points and providing uncertainty estimates. The system should explicitly handle the case of shapes with missing data, inferring missing data points and providing uncertainty estimates. The system should explicitly handle the case of shapes with noisy data, using robust statistical methods to filter out noise and provide uncertainty estimates. The system should explicitly handle the case of shapes with varying levels of complexity, regularity, noise, and ambiguity. The system should explicitly handle the case of shapes with multiple connected components, classifying each component separately. The system should explicitly handle the case of shapes with varying levels of complexity, regularity, noise, and ambiguity. Include \"irregular polygon\" as a valid classification. Calculate a metric for polygon regularity (e.g., standard deviation of side lengths and angles). This metric should be included in the `details` field and used to adjust the confidence score. A high irregularity score should lower the confidence for specific polygon classifications. Ensure probabilities sum to 1. Probabilities should reflect uncertainty. For example, an irregular pentagon should have a higher probability for \"irregular polygon\" and lower probabilities for \"pentagon\" and other specific polygon types. Prioritize determining the number of sides as the first step in polygon classification. Suggest specific algorithms for side length and angle calculation, regularity metric calculation, and number of sides detection.  Employ a multi-stage classification process, starting with broad categories and progressively refining the classification based on calculated geometric properties.  Utilize a combination of geometric analysis and machine learning techniques to improve classification accuracy.  Incorporate contextual information from surrounding text, if available, to aid in disambiguation.  The confidence score should be a weighted average of geometric confidence and contextual confidence (if available).  The system should be robust to variations in vertex order.  The system should be able to handle shapes with multiple connected components, holes, concave polygons, self-intersecting polygons, partially defined shapes, missing data, and noisy data.  The system should be able to handle shapes with varying levels of complexity, regularity, noise, and ambiguity.  The system should explicitly handle cases where the shape is implicitly defined (e.g., a sector might be defined by only a few points on the arc).  The prompt should instruct the model on how to infer missing data points and estimate the complete shape parameters, along with associated confidence scores reflecting the uncertainty.  The system should prioritize geometric property analysis over simple vertex counting, especially for quadrilaterals.  The system should explicitly detail a hierarchical classification strategy for quadrilaterals, checking for properties like parallel sides, equal side lengths, right angles, and diagonal lengths to distinguish between parallelograms, rectangles, squares, trapezoids, and irregular quadrilaterals.  The system should output a probability distribution over all possible classes, even if the highest probability is low.  The system should handle plural forms of shape names by stemming the output to its singular form before comparison.  Preprocess the input string by removing leading and trailing whitespace, removing punctuation marks (.,!?\"), and replacing multiple spaces with single spaces.  The system should be robust to variations in vertex order.  The system should explicitly handle cases where the shape is implicitly defined (e.g., a sector might be defined by only a few points on the arc).  The prompt should instruct the model on how to infer missing data points and estimate the complete shape parameters, along with associated confidence scores reflecting the uncertainty.  The system should prioritize geometric property analysis over simple vertex counting, especially for quadrilaterals.  The system should explicitly detail a hierarchical classification strategy for quadrilaterals, checking for properties like parallel sides, equal side lengths, right angles, and diagonal lengths to distinguish between parallelograms, rectangles, squares, trapezoids, and irregular quadrilaterals.  The system should output a probability distribution over all possible classes, even if the highest probability is low.  The system should handle plural forms of shape names by stemming the output to its singular form before comparison.  Preprocess the input string by removing leading and trailing whitespace, removing punctuation marks (.,!?\"), and replacing multiple spaces with single spaces.  The system should be robust to variations in vertex order.  The system should explicitly handle cases where the shape is implicitly defined (e.g., a sector might be defined by only a few points on the arc).  The prompt should instruct the model on how to infer missing data points and estimate the complete shape parameters, along with associated confidence scores reflecting the uncertainty.  The system should prioritize geometric property analysis over simple vertex counting, especially for quadrilaterals.  The system should explicitly detail a hierarchical classification strategy for quadrilaterals, checking for properties like parallel sides, equal side lengths, right angles, and diagonal lengths to distinguish between parallelograms, rectangles, squares, trapezoids, and irregular quadrilaterals.  The system should output a probability distribution over all possible classes, even if the highest probability is low.  The system should handle plural forms of shape names by stemming the output to its singular form before comparison.  Preprocess the input string by removing leading and trailing whitespace, removing punctuation marks (.,!?\"), and replacing"

BIG_penguins_in_a_table_prompt_gemini = "provided information, even if incomplete. Prioritize providing partial answers or relevant information if a complete answer is impossible.  If data is missing, state what's missing and how it impacts the answer; attempt estimation using appropriate methods (mean, median, mode, interpolation), clearly stating assumptions and quantifying uncertainty.  The confidence score should reflect the certainty of the *provided* information, not its completeness.  Always attempt to answer, even with low confidence, providing a detailed explanation of uncertainties."

BIG_temporal_sequences_prompt_gemini = "The primary goal is accurate time range prediction; the output should be a concise time range in one of the following formats: HH:MM-HH:MM, HH:MM AM/PM - HH:MM AM/PM, or a numerical representation like 17:00-19:00. The response must contain *only* the time range; no additional explanatory text or preambles are permitted. Use a 12-hour clock with AM/PM indicators unless the input uses a 24-hour clock, in which case use the 24-hour format consistently. If no time range can be determined with certainty, return 'N/A'. Examples of acceptable outputs: 17:00-19:00, 5:00 PM - 7:00 PM, 05:00-07:00. Prioritize brevity; shorter, accurate responses are preferred."

BIG_epistemic_reasoning_prompt_gemini = ""

BIG_causal_judgment_prompt_gemini = ""

BIG_object_counting_prompt_gemini = ""